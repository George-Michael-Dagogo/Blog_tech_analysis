{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from fake_useragent import UserAgent\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from sqlalchemy import create_engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Punch Newspaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def punch_news():\n",
    "    options = Options()\n",
    "    options.headless = True\n",
    "    ua = UserAgent()\n",
    "    userAgent = ua.random\n",
    "    options.add_argument(f'user-agent={userAgent}')\n",
    "\n",
    "    c = DesiredCapabilities.CHROME\n",
    "    c[\"pageLoadStrategy\"] = \"none\"\n",
    "    #set chromodriver.exe path\n",
    "    driver = webdriver.Chrome(r'C:\\Users\\HP\\Downloads\\news\\News_station_analysis\\chromedriver.exe',desired_capabilities=c,options=options)\n",
    "    #explicit wait\n",
    "    w = WebDriverWait(driver, 20)\n",
    "    #launch URL\n",
    "    driver.get(\"https://punchng.com/topics/news/\")\n",
    "    driver.implicitly_wait(20)\n",
    "    time.sleep(3)\n",
    "    #driver.implicitly_wait(20)\n",
    "    #expected condition\n",
    "    w.until(EC.presence_of_element_located((By.CLASS_NAME, 'post-title')))\n",
    "    #JavaScript Executor to stop page load\n",
    "\n",
    "    driver.execute_script(\"window.stop();\")\n",
    "    print('First huddle')\n",
    "\n",
    "    content = []\n",
    "    contents =driver.find_elements_by_class_name(\"post-title\")\n",
    "    for con in contents:\n",
    "        cont = con.get_attribute('innerHTML')\n",
    "        content.append(cont)\n",
    "    af = pd.DataFrame(content,columns =['content'])\n",
    "    af.content = af.content.apply(lambda x: x.replace('<a href=', ''))\n",
    "    af.content = af.content.apply(lambda x: x.replace('</a>', ''))\n",
    "    af.content = af.content.apply(lambda x: x.replace('>', '|'))\n",
    "    af = af['content'].str.split(\"|\",n = 3, expand = True)\n",
    "    af.columns = ['link','title']\n",
    "    af = af.drop_duplicates(subset=[\"link\"], keep='first')\n",
    "    print('Second huddle')\n",
    "    full_contents = []\n",
    "    dates = []\n",
    "    by = []\n",
    "    def all_news(ev):\n",
    "        h = WebDriverWait(driver, 20)\n",
    "        full = []\n",
    "        timed = []\n",
    "        print('Pages extraction in progress')\n",
    "\n",
    "        driver.get(ev)\n",
    "        time.sleep(4)\n",
    "        driver.implicitly_wait(20)\n",
    "        h.until(EC.presence_of_element_located((By.CLASS_NAME, 'post-content')))\n",
    "        #JavaScript Executor to stop page load\n",
    "        driver.execute_script(\"window.stop();\")\n",
    "        full_content = driver.find_elements_by_class_name(\"post-content\")\n",
    "        for conten in full_content:\n",
    "            co = conten.get_attribute('innerText')\n",
    "            co1 = co.replace('\\n\\n',' ')\n",
    "            co2 = co1.replace('\\n',' ')\n",
    "            co3 = co2.split(',', 1)\n",
    "            full.append(co3)\n",
    "\n",
    "        date = driver.find_elements_by_class_name(\"col-lg-4\")\n",
    "        for dat in date:\n",
    "            dat1= dat.get_attribute('innerText')\n",
    "            dat2 = dat1.replace('By\\xa0\\n','')\n",
    "            timed.append(dat2)\n",
    "        full_contents.append(full[0])\n",
    "        dates.append(timed[0])\n",
    "        by.append(timed[1])\n",
    "\n",
    "    m =af.link.to_list()\n",
    "    m =  [item.replace('\"', '') for item in m]\n",
    "    for o in m:\n",
    "        all_news(o)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    aa = pd.DataFrame({'Title':af.title,'Full_content': full_contents,'Date':dates,'Author':by,'Source_link':af.link})\n",
    "    ff  =aa['Full_content'].apply(lambda x: ' '.join(dict.fromkeys(x).keys()))#unlist the full_content column\n",
    "\n",
    "    aa['Words_count'] = ff.str.split().str.len()#counts the full_content\n",
    "    n = open(\"negative-words.txt\", \"r\")\n",
    "    p = open(\"positive-words.txt\", \"r\")\n",
    "    n_word = n.read()\n",
    "    p_word = p.read()\n",
    "    n.close()\n",
    "    p.close()\n",
    "    n_word=n_word.replace('\\n',',')\n",
    "    n_word = re.sub(\"[^\\w]\", \" \", n_word).split()\n",
    "    p_word=p_word.replace('\\n',',')\n",
    "    p_word = re.sub(\"[^\\w]\", \" \", p_word).split()\n",
    "    aa['Full_content'] = ff\n",
    "    #df['word_overlap'] = [set(x[0].split()) & set(x[1].split()) for x in df.values]\n",
    "    def negative_words(x):\n",
    "        negative_score = 0\n",
    "        for word in n_word:\n",
    "            if word in x:\n",
    "                negative_score += 1\n",
    "        return negative_score\n",
    "\n",
    "    def positive_words(x):\n",
    "        positive_score = 0\n",
    "        for word in p_word:\n",
    "            if word in x:\n",
    "                positive_score += 1\n",
    "        return positive_score\n",
    "    aa['Negative_words'] = aa['Full_content'].apply(lambda x : negative_words(x))\n",
    "    aa['Positive_words'] = aa['Full_content'].apply(lambda x : positive_words(x))\n",
    "    aa['Sentence_count'] = aa['Full_content'].str.count('[\\w][\\.!\\?]')\n",
    "    aa['Sentiment'] = round((aa['Positive_words'] - aa['Negative_words']) / aa['Words_count'], 2)\n",
    "    aa['News_type'] = ['Bad News' if x < 0 else 'Good News' if x > 0 else 'Neutral' for x in aa.Sentiment]\n",
    "    engine = create_engine('sqlite:///news.db')\n",
    "    aa.to_sql('punch_data', engine, if_exists='append', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First huddle\n",
      "Second huddle\n",
      "Pages extraction in progress\n",
      "Pages extraction in progress\n",
      "Pages extraction in progress\n",
      "Pages extraction in progress\n",
      "Pages extraction in progress\n",
      "Pages extraction in progress\n",
      "Pages extraction in progress\n",
      "Pages extraction in progress\n",
      "Pages extraction in progress\n",
      "Pages extraction in progress\n",
      "Pages extraction in progress\n",
      "Pages extraction in progress\n",
      "Pages extraction in progress\n",
      "Pages extraction in progress\n",
      "Pages extraction in progress\n",
      "Pages extraction in progress\n",
      "Pages extraction in progress\n",
      "Pages extraction in progress\n",
      "Pages extraction in progress\n",
      "Pages extraction in progress\n",
      "Pages extraction in progress\n",
      "Pages extraction in progress\n",
      "Pages extraction in progress\n",
      "Pages extraction in progress\n",
      "Pages extraction in progress\n",
      "Pages extraction in progress\n",
      "Pages extraction in progress\n",
      "Pages extraction in progress\n",
      "Pages extraction in progress\n",
      "Pages extraction in progress\n",
      "Pages extraction in progress\n",
      "Pages extraction in progress\n"
     ]
    }
   ],
   "source": [
    "punch_news()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanguard Newspaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First huddle\n",
      "going\n",
      "going\n",
      "going\n",
      "going\n",
      "going\n",
      "going\n",
      "going\n",
      "going\n",
      "going\n",
      "going\n",
      "going\n",
      "going\n",
      "going\n",
      "going\n",
      "going\n",
      "going\n",
      "going\n",
      "going\n",
      "going\n",
      "going\n"
     ]
    }
   ],
   "source": [
    "def vanaguard_news():\n",
    "    options = Options()\n",
    "    ua = UserAgent()\n",
    "    userAgent = ua.random\n",
    "    options.add_argument(f'user-agent={userAgent}')\n",
    "    options.headless = True\n",
    "    c = DesiredCapabilities.CHROME\n",
    "    c[\"pageLoadStrategy\"] = \"none\"\n",
    "    #set chromodriver.exe path\n",
    "\n",
    "    driver = webdriver.Chrome(r'C:\\Users\\HP\\Downloads\\news\\News_station_analysis\\chromedriver.exe',desired_capabilities=c,options=options)\n",
    "    #explicit wait\n",
    "\n",
    "    driver.get(\"https://www.vanguardngr.com/news/\")\n",
    "    #explicit wait\n",
    "    w = WebDriverWait(driver, 20)\n",
    "    #launch URL\n",
    "    #driver.get(\"https://www.vanguardngr.com/category/headlines/\")\n",
    "    driver.implicitly_wait(20)\n",
    "    time.sleep(3)\n",
    "    #driver.implicitly_wait(20)\n",
    "    #expected condition\n",
    "    w.until(EC.presence_of_element_located((By.CLASS_NAME, 'entry-title')))\n",
    "    #JavaScript Executor to stop page load\n",
    "\n",
    "    driver.execute_script(\"window.stop();\")\n",
    "    print('First huddle')\n",
    "\n",
    "    content = []\n",
    "    contents =driver.find_elements_by_class_name(\"entry-title\")\n",
    "    for con in contents:\n",
    "        cont = con.get_attribute('innerHTML')\n",
    "        content.append(cont)\n",
    "\n",
    "    af = pd.DataFrame(content,columns =['content'])\n",
    "    af.content = af.content.apply(lambda x: x.replace('<a href=\"', ''))\n",
    "    af = af.iloc[1:]\n",
    "    af = af.reset_index()\n",
    "    af.content = af.content.apply(lambda x: x.replace('rel=\"bookmark\">', ' | '))\n",
    "    af.content = af.content.apply(lambda x: x.replace('</a>', ' '))\n",
    "    af = af['content'].str.split(\"|\",n = 3, expand = True)\n",
    "    af.columns = ['News_link','Title']\n",
    "    sd = af.head(20)\n",
    "    driver.quit()\n",
    "    full_contents = []\n",
    "    dates = []\n",
    "    datetime = []\n",
    "    genres = []\n",
    "    def all_news(ev):\n",
    "        options = Options()\n",
    "        options.headless = True\n",
    "        ua = UserAgent()\n",
    "        userAgent = ua.random\n",
    "        options.page_load_strategy = 'eager'\n",
    "        options.add_argument(f'user-agent={userAgent}')\n",
    "        driver = webdriver.Chrome(r'C:\\Users\\HP\\Downloads\\news\\News_station_analysis\\chromedriver.exe',options=options)\n",
    "        driver.get(ev)\n",
    "        driver.implicitly_wait(20)\n",
    "        time.sleep(10)\n",
    "\n",
    "        full_content = driver.find_elements_by_class_name(\"entry-content\")\n",
    "        for conten in full_content:\n",
    "            co = conten.get_attribute('innerText')\n",
    "            co1 = co.replace('\\n\\n','')\n",
    "            co2 = co1.replace('Subscribe for latest Videos','')\n",
    "            #co3 = co2[co2.find('|'):] #deletes anything before the |\n",
    "            co3 = co2.replace('\\n',' ')\n",
    "            co4 = co3.split(',',1)\n",
    "            full_contents.append(co4)\n",
    "\n",
    "        date = driver.find_elements_by_class_name(\"entry-date.published.updated\")\n",
    "        for dat in date:\n",
    "            date= dat.get_attribute('innerText')\n",
    "            dates.append(date)\n",
    "            tim= dat.get_attribute('dateTime')\n",
    "            datetime.append(tim)\n",
    "\n",
    "        genre = driver.find_elements_by_xpath(\"\"\"//*[@id=\"main\"]/header/span/a\"\"\")\n",
    "        for gen in genre:\n",
    "            gen1= gen.get_attribute('innerText')\n",
    "            #gen2 = gen1.replace('POSTED IN\\n','')\n",
    "            genres.append(gen1)\n",
    "        print('going')\n",
    "        driver.quit()\n",
    "\n",
    "    a = sd.News_link.to_list()\n",
    "    a =  [item.replace('\"  ', '') for item in a]\n",
    "    #a = ['https://www.vanguardngr.com/2022/12/mavins-marks-10th-anniversary-with-new-album/']\n",
    "    for i in a:\n",
    "        all_news(i)\n",
    "\n",
    "    ss = pd.DataFrame({'Full_content': full_contents,'Date':dates,'Time_published':datetime})\n",
    "    ff  =ss['Full_content'].apply(lambda x: ' '.join(dict.fromkeys(x).keys()))\n",
    "\n",
    "    ss['Words_count'] = ff.str.split().str.len()\n",
    "    n = open(\"negative-words.txt\", \"r\")\n",
    "    p = open(\"positive-words.txt\", \"r\")\n",
    "    n_word = n.read()\n",
    "    p_word = p.read()\n",
    "    n.close()\n",
    "    p.close()\n",
    "    n_word=n_word.replace('\\n',',')\n",
    "    n_word = re.sub(\"[^\\w]\", \" \", n_word).split()\n",
    "    p_word=p_word.replace('\\n',',')\n",
    "    p_word = re.sub(\"[^\\w]\", \" \", p_word).split()\n",
    "    #df['word_overlap'] = [set(x[0].split()) & set(x[1].split()) for x in df.values]\n",
    "    def negative_words(x):\n",
    "        negative_score = 0\n",
    "        for word in n_word:\n",
    "            if word in x:\n",
    "                negative_score += 1\n",
    "        return negative_score\n",
    "\n",
    "    def positive_words(x):\n",
    "        positive_score = 0\n",
    "        for word in p_word:\n",
    "            if word in x:\n",
    "                positive_score += 1\n",
    "        return positive_score\n",
    "    ss['Full_content'] = ff\n",
    "    ss['News_genre'] = genres\n",
    "    ss['Negative_words'] = ss['Full_content'].apply(lambda x : negative_words(x))\n",
    "    ss['Positive_words'] = ss['Full_content'].apply(lambda x : positive_words(x))\n",
    "    ss['Sentence_count'] = ss['Full_content'].str.count('[\\w][\\.!\\?]')\n",
    "\n",
    "    ss['Sentiment'] = round((ss['Positive_words'] - ss['Negative_words']) / ss['Words_count'], 2)\n",
    "    ss['News_type'] = ['Bad News' if x < 0 else 'Good News' if x > 0 else 'Neutral' for x in ss.Sentiment]\n",
    "    va = pd.concat([sd,ss], axis=1)\n",
    "    engine = create_engine('sqlite:///news.db')\n",
    "    va.to_sql('vanguard_data', engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
